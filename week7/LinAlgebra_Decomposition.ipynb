{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9s_hbe015UpH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "smxEcUil5YXg"
   },
   "source": [
    "# Matrix Decomposition\n",
    "\n",
    "## Outline\n",
    "* Basic interpretation of matrix\n",
    "* Basic definition\n",
    "* Special matrices\n",
    "* LU and QR decompositions\n",
    "* Cholesky decomposition \n",
    "* Eigenvalue decomposition\n",
    "* Singular Value Decomposition\n",
    "  * Linear Regression \n",
    "* Principal Component Analysis**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "we0BPA8ldbMg"
   },
   "source": [
    "## Basic Interpretations\n",
    "* Matrices are both operands and operators at the same time.\n",
    "* Matrix multipliction has several interpretations\n",
    "  * element view\n",
    "  * row view\n",
    "  * column view\n",
    "* Examples of matrices as operators\n",
    "\n",
    "## Basic Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4NoUj4rAxerK"
   },
   "source": [
    "### Eigenvector and Eigenvalues\n",
    "**Definition** \n",
    "* An **eigenvector-eigenvalue pair** of a square matrix $A$ is a pair of a vector and scalar $(v,λ)$ for which $Av=λv$. \n",
    "* The **spectrum** of a matrix A is the set of all its eigenvalues.\n",
    "\n",
    "We make the following observations.\n",
    "1. The above definition implies that $(A−λI)v=0$.\n",
    "2. The vector v can only be a solution of $(A−λI)v=0$ if $dimnull(A−λI)≥1$, implying that $(A−λI)$ is a singular matrix.\n",
    "3. If v is an eigenvector of $A$, then so is $cv$ for all constant $c$. (with the same eigenvalue).\n",
    "\n",
    "\n",
    "### Determinant\n",
    "To define determinant, we first define **permutation** and **inversion of permutation**.\n",
    "\n",
    "**Definition**\n",
    "* A **permutation** of order $n$ is a one to one and onto function from ${1,…,n}$ to ${1,…,n}$. \n",
    "  * The product of two permutations $πσ$ is defined to be their function composition $π∘σ$. \n",
    "  * The set of all permutations of order n is denoted $\\mathfrak{S}_n$.\n",
    "\n",
    "**Definition** \n",
    "* A pair $a,b \\in {1,…,n}$ and $a \\ne b$ is an **inversion of a permutation** $σ \\in \\mathfrak{S}_n$ if $a < b$ but $σ(b) < σ(a)$. \n",
    "  * In other words, the function $σ$ reverses the natural order of $a$,$b$. \n",
    "  * The number of inversions of a permutation is called its **parity**. \n",
    "  * Permutations with an even parity are called even and permutations with an odd parity are called odd. \n",
    "  * The sign of a permutation $σ$, denoted $s(σ)$, equals 1 if $σ$ is even and -1 if $σ$ is odd.\n",
    "\n",
    "**Definition** \n",
    "* The **determinant** of a square matrix $A \\in R^{n×n}$ is defined as\n",
    "$$\n",
    "detA= \\sum_{σ \\in \\mathfrak{S}_n} s(σ) A_{1,σ(1)} A_{2,σ(2)} ⋯ A_{n,σ(n)}.\n",
    "$$\n",
    "\n",
    "**Proposition**\n",
    "* The determinant of a triangular matrix is the product of its diagonal elements.\n",
    "\n",
    "**Proof**\n",
    "* All permutation, except the one without change, will include at least one 0.\n",
    "\n",
    "**Proposition**\n",
    "* The following properties hold\n",
    "1. $detI=1$.\n",
    "2. $detA$ is a linear function of the $j-$column vector $v=(A_{1j},…,A_{nj})$ assuming other columns are held fixed.\n",
    "3. If $A′$ is obtained from $A$ by interchanging two columns then $detA=−detA′$.\n",
    "4. If A has two equal columns then $detA=0$.\n",
    "5. $det(A)=det(A^⊤)$.\n",
    "\n",
    "**Proposition**\n",
    "* A square matrix is singular if and only if its determinant is zero.\n",
    "\n",
    "**Definition**\n",
    "* The characteristic polynomial of a square matrix A is the function\n",
    "$$f(λ)=det(λI−A), \\quad λ∈R$$.\n",
    "\n",
    "**Observation**\n",
    "* Since if $\\lambda$ is an eigenvalue, then $A-\\lambda I$ must be singular.  So if $\\lambda$ is an eigenvalue, we have\n",
    "$$f(λ)=det(λI−A) = 0.$$\n",
    "That is, eigenvalues are the roots of the characteristic polynomial $f(\\lambda)$.\n",
    "* If $f(\\lambda)$ is a polynomial of power $n$, and $\\lambda_1, ..., \\lambda_n$ are the $n$ roots of $f(x)$, then we have\n",
    "$$ f(\\lambda) = \\prod_i (\\lambda - \\lambda_i)$$\n",
    "\n",
    "### Trace\n",
    "**Definition**\n",
    "* The **trace** of a square matrix trace(A) is the sum of its diagonal elements.\n",
    "\n",
    "**Proposition**\n",
    "* The following properties hold:\n",
    "$$trace(A+B)=trace(A)+trace(B)$$\n",
    "$$trace(AB)=trace(BA)$$.\n",
    "\n",
    "**Proof:**\n",
    "$$trace(AB)= \\sum_k \\sum_j A_{kj} B_{jk} \n",
    "=\\sum_k \\sum_j B_{kj} A_{jk} = trace(BA)$$\n",
    "\n",
    "**Definition: Frobenius norm**\n",
    "* The **Frobenius norm** of a matrix A is the Euclidean norm of the vector obtained by concatenating the matrix columns into one long vector\n",
    "$$ \\parallel A \\parallel_F = \\sqrt{ \\sum_i \\sum_j A^2_{ij}} $$.\n",
    "\n",
    "**Observation**\n",
    "* Since the diagonal elements of $AA^⊤$ are the sum of squares of the rows of A, and the diagonal elements of $A^⊤A$ are the sum of squares of the columns of A, we have\n",
    "$$ \\parallel A \\parallel_F = \\sqrt{ trace(A A^T) }\n",
    "= \\sqrt{trace(A^T A)} $$\n",
    "\n",
    "**Proposition**\n",
    "* For any matrix $A$ and any orthogonal matrix $U$,\n",
    "$$\\parallel UA \\parallel_F= \\sqrt{(UA)^T(UA)} = \\sqrt{A^TU^T UA}   =\\parallel A \\parallel_F$$.\n",
    "\n",
    "**proposition** (Important!)\n",
    "* If $A$ is a square matrix with eigenvalues $λ_i, i=1,…,n$\n",
    "$$traceA=\\sum_{i=1}^n λ_i $$\n",
    "$$detA = \\prod_{i=1}^n λ_i$$.\n",
    "\n",
    "**Proof**\n",
    "* **Value of determinant**: Since \n",
    "$$f(λ)=det(λI−A)= \\prod_i (\\lambda - \\lambda_i)$$\n",
    "If we substitute $\\lambda = 0$ into the equation, we have\n",
    "$$ f(0) = det(-A) = (-1)^n \\prod_{i=1}^n \\lambda_i$$\n",
    "And\n",
    "$$ det(A) = \\prod_{i=1}^n \\lambda_i $$\n",
    "* **Value of trace**: First, we notice that for $det(λI−A)$ is a polynomial of $\\lambda$ of order $n$, and the only time we can get $\\lambda^{n-1}$ is from the expansion of the product of the diagonal $\\prod_{i=1}^n(\\lambda - A_{ii})$.  If we expand both sides of\n",
    "$$ det(λI−A)= \\prod_i (\\lambda - \\lambda_i),$$\n",
    "and compare the coefficient of the $\\lambda^{n-1}$ term on both sides, we will have\n",
    "$$ trace(A) = \\sum_i A_{ii} = \\sum_i \\lambda_i $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mtf25Yg347lp"
   },
   "source": [
    "## Special Matrices: Defintions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "szCIpDE4RKlq"
   },
   "source": [
    "\n",
    "**Definition 1**\n",
    "* A real matrix $A$ is a **symmetric matrix** if it equals to its own transpose, i.e., $A = A^T$.\n",
    "\n",
    "**Definition 2**\n",
    "* A complex matrix $A$ is a **hermitian matrix** if it equals to its own **complex conjugate transpose**, i.e., $A = A^H$.\n",
    "\n",
    "**Definition 3**\n",
    "* A real matrix $Q$ is an **orthogonal matrix** if the inverse of $Q$ equals to the transpose of $Q$, i.e., $Q^{−1} = Q^T$\n",
    "  * From the definion, we have $QQ^T = Q^TQ = I$\n",
    "  * In addition, expressing $Q=\\begin{bmatrix} q_1 &q_2 &... &q_n\\end{bmatrix}$, we know $\\{q_1, q_2, ..., q_n\\}$ is a set of orthonogal unit vectors that span $R^n$\n",
    "\n",
    "**Definition 4** \n",
    "* A complex matrix $U$ is a **unitary matrix** if the inverse of $U$ equals the complex conjugate transpose of U, i.e., $U^{−1} = U^H$\n",
    "  * Furthermore, $UU^H = U^HU = I$.\n",
    "\n",
    "**Definition 5** \n",
    "* A matrix $A ∈ R^{n×n}$ is **positive definite** if $x^TAx > 0$ for all nonzero vector $x ∈ R^n$.\n",
    "  * Positive definite matrices have positive definite principal sub-matrices \n",
    "  * All the diagonal entries are positive.\n",
    "\n",
    "**Definition 6**\n",
    "* Suppose $S ⊆ R^n$ be a subspace with orthonormal basis $V = (v_1, . . . , v_k)$, $P = V^TV ∈ R^{n×n}$\n",
    "is the orthogonal projection matrix onto S such that $range(P) = S$, $P^2 = P$, and $P^T = P$. $P$ is unique for subspace $S$.\n",
    "\n",
    "Note that hermitian matrix and unitary matrix for complex matrices are the counterparts of symmetric matrix and orthogonal matrix for real matrices.  In the following discussion.  We discuss only about real matrices.  The principles for complex matrices can be obtained by substituing hermitian and unitary matrix for symmetric and orthogonal matrix in the discussin.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IkwkmE0J51s2"
   },
   "source": [
    "## Matrix Decomposition: What is it?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uiAlADDc7IJH"
   },
   "source": [
    "* What: convert a matrix into multiplication of a number of matrices\n",
    "  * $M = M_1 \\times M_2 \\times … \\times M_n$\n",
    "  * Usually $n$ is a small numbers: 2 or 3\n",
    "  * There are many different way to decompose a matrix\n",
    "    * Also called **matrix factorization**\n",
    "* Purpose:\n",
    "    * Some operation can be performed on the decomposed matrices more efficiently than performed on the original matrix\n",
    "\n",
    "\n",
    "###LU Decomposition\n",
    "\n",
    "* $A = LU$ (ideal case)\n",
    "* $A = PLU$ ($P$ is a permutation matrix)\n",
    "* $A = LDV$ \n",
    "\n",
    "where\n",
    "* $L$ is a lower-triangle matrix, with diagonal all = 1\n",
    "* $U$ is a upper-triangle matrix\n",
    "* $D$ is a diagonal matrix (all 0 except on diagonal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 768,
     "status": "ok",
     "timestamp": 1571801825268,
     "user": {
      "displayName": "ML Lo",
      "photoUrl": "",
      "userId": "11511065297651515564"
     },
     "user_tz": -480
    },
    "id": "hRlwUVpN7fX1",
    "outputId": "733eb030-4d52-435c-965d-c0db29313e32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5  2  3]\n",
      " [ 4  5  6]\n",
      " [ 7 17  9]]\n",
      "[[0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]]\n",
      "[[1.         0.         0.        ]\n",
      " [0.71428571 1.         0.        ]\n",
      " [0.57142857 0.46478873 1.        ]]\n",
      "[[  7.          17.           9.        ]\n",
      " [  0.         -10.14285714  -3.42857143]\n",
      " [  0.           0.           2.45070423]]\n",
      "[[ 5.  2.  3.]\n",
      " [ 4.  5.  6.]\n",
      " [ 7. 17.  9.]]\n"
     ]
    }
   ],
   "source": [
    "# LU decomposition\n",
    "from numpy import array\n",
    "from scipy.linalg import lu\n",
    "# define a square matrix\n",
    "A = array([[5, 2, 3], [4, 5, 6], [7, 17, 9]])\n",
    "print(A)\n",
    "# LU decomposition\n",
    "P, L, U = lu(A)\n",
    "print(P)\n",
    "print(L)\n",
    "print(U)\n",
    "# reconstruct\n",
    "B = P.dot(L).dot(U)\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "53TGYyIb7zac"
   },
   "source": [
    "###QR Decomposition\n",
    "\n",
    "* The QR decomposition is for m x n matrices\n",
    "* $A = QR$\n",
    "* $Q$ a (full) matrix with the size $m \\times m$, and \n",
    "* $R$ is an upper triangle matrix with the size $m \\times n$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 844,
     "status": "ok",
     "timestamp": 1570983975907,
     "user": {
      "displayName": "ML Lo",
      "photoUrl": "",
      "userId": "11511065297651515564"
     },
     "user_tz": -480
    },
    "id": "xdigih1n8B3q",
    "outputId": "285ebbfa-ca54-4388-a884-00c6ede3a245"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]\n",
      " [5 6]]\n",
      "[[-0.16903085  0.89708523  0.40824829]\n",
      " [-0.50709255  0.27602622 -0.81649658]\n",
      " [-0.84515425 -0.34503278  0.40824829]]\n",
      "[[-5.91607978 -7.43735744]\n",
      " [ 0.          0.82807867]\n",
      " [ 0.          0.        ]]\n",
      "[[1. 2.]\n",
      " [3. 4.]\n",
      " [5. 6.]]\n"
     ]
    }
   ],
   "source": [
    "# QR decomposition\n",
    "from numpy import array\n",
    "from numpy.linalg import qr\n",
    "# define a 3x2 matrix\n",
    "A = array([[1, 2], [3, 4], [5, 6]])\n",
    "print(A)\n",
    "# QR decomposition\n",
    "Q, R = qr(A, 'complete')\n",
    "print(Q)\n",
    "print(R)\n",
    "# reconstruct\n",
    "B = Q.dot(R)\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DmU24ltr8eCo"
   },
   "source": [
    "###Cholesky Decomposition\n",
    "\n",
    "* Assume matrix $A$ is a square symmetric matrices where all eigenvalues are greater than zero\n",
    "  * That is, a **positive definite** symmetric matrices\n",
    "* Then $A$ can be decomposed into $A = L L^T$\n",
    "  * Where $L$ is a lower triangular matrix\n",
    "  * By definition, $L^T$ must be an upper triangular matrix\n",
    "* Why useful? Many matrix that occurs in nature are symmetric\n",
    "* Cholesky decomposition is used for \n",
    "  * solving linear least squares for linear regression, \n",
    "  * simulation \n",
    "  * optimization methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 988,
     "status": "ok",
     "timestamp": 1570984244656,
     "user": {
      "displayName": "ML Lo",
      "photoUrl": "",
      "userId": "11511065297651515564"
     },
     "user_tz": -480
    },
    "id": "RFuXnu3d9FQp",
    "outputId": "fc5e15df-e40c-4bab-f778-b9126d5b4797"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 1 1]\n",
      " [1 2 1]\n",
      " [1 1 2]]\n",
      "[[1.41421356 0.         0.        ]\n",
      " [0.70710678 1.22474487 0.        ]\n",
      " [0.70710678 0.40824829 1.15470054]]\n",
      "[[2. 1. 1.]\n",
      " [1. 2. 1.]\n",
      " [1. 1. 2.]]\n"
     ]
    }
   ],
   "source": [
    "# Cholesky decomposition\n",
    "from numpy import array\n",
    "from numpy.linalg import cholesky\n",
    "# define a 3x3 matrix\n",
    "A = array([[2, 1, 1], [1, 2, 1], [1, 1, 2]])\n",
    "print(A)\n",
    "# Cholesky decomposition\n",
    "L = cholesky(A)\n",
    "print(L)\n",
    "# reconstruct\n",
    "B = L.dot(L.T)\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ztMOPCFJePbp"
   },
   "source": [
    "### Eigenvalue Decomposition (Eigen Decomposition)\n",
    "Assume $A$ is an $n \\times n$ matrix for which $n$ eigenvalue and eigenvector pairs exist, then we have:\n",
    "$$Au = λ_iu, \\quad \\text{for } i=1, ..., n$$\n",
    "Putting all these n equations together, into matrix form, we have:\n",
    "$$AU = U\\Lambda$$\n",
    "Where $U$ is the matrix of all eigen vector, hence is a orthnormal matrix, and $\\Lambda$ is a diagonal matrix with eigenvalues on its diagonal matrix. For an orthonormal matrix $U^{-1} = U^T$, therefore we have\n",
    "$$ A = U \\Lambda U^{-1} = U\\lambda U^T$$\n",
    "\n",
    "#### Diagonalization\n",
    "From $ A = U\\lambda U^T$, we have \n",
    "$$ \\lambda = U^T A U$$\n",
    "This is sometimes referred to as the **diagonalization of** $A$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2470,
     "status": "ok",
     "timestamp": 1571299426450,
     "user": {
      "displayName": "ML Lo",
      "photoUrl": "",
      "userId": "11511065297651515564"
     },
     "user_tz": -480
    },
    "id": "9ug6ckzfu-MB",
    "outputId": "f253a346-15a4-4bd7-998f-91f219fd4640"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 2. 3.]\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "w, v = LA.eig(np.diag((1, 2, 3)))  # a diagonal matrix [[1,0,0],[0,2,0],[0,0,3]]\n",
    "print(w)\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LkaKpB7dmqSx"
   },
   "source": [
    "#### Positive (semi-)definite matrix\n",
    "\n",
    "A matrix $A$ is **positive definite**, if for all vector $x \\in \\mathbb{R^n}$,\n",
    "$$ x^T A x > 0$$\n",
    "$A$ is **positive semi-definite**, if for all vector $x \\in \\mathbb{R^n}$,\n",
    "$$ x^T A x \\ge 0$$\n",
    "\n",
    "For a **positive semi-definite** $A$, the following three statements are equivalent:\n",
    "1. For all vector $x \\in \\mathbb{R^n}$,\n",
    "$$ x^T A x \\ge 0$$\n",
    "2. There exists some matrix $X$, such that \n",
    "$$ A = X X^T$$\n",
    "3. For all eigenvalue $\\lambda$ of $A$ \n",
    "$$ \\lambda \\ge 0$$\n",
    "\n",
    "The eigen decomposition is quite useful for proving the equivalence of the above three statements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ERrpSJxqWZew"
   },
   "source": [
    "### Singular Value Decomposition (SVD)\n",
    "Let $M$ be an $m \\times n$ matrix, $M$ can be decomposed into the form\n",
    "$$\\mathbf {M} =\\mathbf {U} {\\boldsymbol {\\Sigma }}\\mathbf {V} ^{T}$$\n",
    "where\n",
    "* $U$ is an $m \\times m$ orthonormal matrix over K \n",
    "* $\\Sigma$ is a diagonal $m × n$ matrix with non-negative real numbers on the diagonal\n",
    "* $V$ is an $n \\times n$ unitary matrix \n",
    "\n",
    "The diagonal entries $\\sigma_i$ of $\\Sigma$ are known as the singular values of $M$. \n",
    "  * A common convention is to list the singular values in descending order. \n",
    "  * In this case, the diagonal matrix, $\\Sigma$, is uniquely determined by $M$ (though not the matrices $U$ and $V$ if $M$ is not square, see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 158
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 749,
     "status": "ok",
     "timestamp": 1571802602261,
     "user": {
      "displayName": "ML Lo",
      "photoUrl": "",
      "userId": "11511065297651515564"
     },
     "user_tz": -480
    },
    "id": "zK9fg0j7bDxX",
    "outputId": "874681e4-ab4d-434d-bd47-3eecdb75bab2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.44457749 -0.79729805  0.40824829]\n",
      " [-0.56842568 -0.10112162 -0.81649658]\n",
      " [-0.69227386  0.59505481  0.40824829]]\n",
      "[6.65637106 1.30104737 1.        ]\n",
      "[[-3.22976966e-01 -3.41582927e-01 -3.60188889e-01 -8.05773509e-01]\n",
      " [-8.45982198e-01 -3.10892967e-01  2.24196263e-01  3.70669285e-01]\n",
      " [ 4.08248290e-01 -8.16496581e-01  4.08248290e-01  6.66133815e-16]\n",
      " [ 1.15470054e-01 -3.46410162e-01 -8.08290377e-01  4.61880215e-01]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import array\n",
    "\n",
    "# define a 3x3 matrix\n",
    "A = array([[2, 1, 1, 2], [1, 2, 1, 3], [1, 1, 2, 4]])\n",
    "u, s, vh = np.linalg.svd(A, full_matrices=True)\n",
    "print(u)\n",
    "print(s)\n",
    "print(vh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9UFebD-Y5TaI"
   },
   "source": [
    "## What does it mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NXjZfkdz5apY"
   },
   "source": [
    "### Singular Matrix Decomposition\n",
    "\n",
    "\n",
    "**Recap:**\n",
    "For any matrix $A \\in R^{n \\times m}$, we have\n",
    "$$A=U \\Sigma V^⊤$$\n",
    "where\n",
    "* $U \\in R^{n×n}$ is an orthogonal matrix whose columns are the eigenvectors of $AA^⊤$\n",
    "* $V \\in R^{m×m}$ is an orthogonal matrix whose columns are the eigenvectors of $A^⊤A$\n",
    "* $\\Sigma \\in R^{n×m}$ is an all zero matrix except for the first r diagonal elements $σ_i=\\Sigma_{ii}, i=1,\\ldots,r$ (called **singular values**) that are the square roots of the eigenvalues of $A^⊤A$ and of $AA^⊤$ (these two matrices have the same eigenvalues).\n",
    "\n",
    "We assume above that the singular values are sorted in descending order and the eigenvectors are sorted according to descending order of their eigenvalues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gu-utht2_ljO"
   },
   "source": [
    "#### Prove SVD solution always exists\n",
    "\n",
    "We assume in the proof below that n≥m. The case where $m>n$, we similarly by transposing $A$ into $A^⊤$.\n",
    "\n",
    "**Proof:**\n",
    "* Let $A$ be an $m \\times n$ matrix. We prove that there exist orthogonal matrices, $U$ and $V$ of the appropriate size such that\n",
    "$$U^TAV = \\begin{pmatrix}\n",
    "σ & 0 \\\\\n",
    "0 & 0 \\end{pmatrix}$$\n",
    "where $\\sigma$ is of the form\n",
    "$$ σ = \\begin{pmatrix}\n",
    "σ_1 & & 0 \\\\\n",
    " &... & \\\\\n",
    "0& &  σ_k\n",
    "\\end{pmatrix}$$\n",
    "where $\\sigma_i$ are the singular values of $A$, arranged in order of decreasing size.\n",
    "\n",
    "* Since $A^TA$ is by definition a positive semi-definite matrix, we have\n",
    "$$A^TAv_i = σ^2_i v_i$$ \n",
    "we arrange the eigenvalues in decreasing order, and have\n",
    "$$σ^2_i \\begin{cases}\n",
    "> 0 &for\\; i = 1, · · · , k, (σ_i > 0)\\\\\n",
    "= 0 &for\\; i > k\n",
    "\\end{cases}$$\n",
    "Note that, for $i > k$, we also have $Av_i = 0$.\n",
    "* For $i = 1, · · · , k$, we define $u_i \\in F^m$ as \n",
    "$$u_i ≡ \\frac{Av_i}{σ_i}$$\n",
    "Equivalently, we can write $Av_i = σ_iu_i$.  \n",
    "* Now\n",
    "$$u_i^T u_j  = (σ^{−1}_i Av_i)^T (σ^{−1}_j Av_j)\\\\\n",
    "=σ^{−1}_i σ^{−1}_j (v_i^T A^T A v_j) \\\\\n",
    "=σ^{−1}_i σ^{−1}_j (v_i^T σ^2_j v_j) \\\\\n",
    "=\\frac{σ_j}{σ_i} v_i^T v_j = δ_{ij} .\n",
    "$$\n",
    "Thus $\\{u_i\\}^k_{i=1}$ is an orthonormal set of vectors in $F^m$. \n",
    "* Now, \n",
    "$$AA^Tu_i = \\frac{1}{σ_i} AA^TAv_i = \\frac{1}{σ_i} A σ^2_i v_i = σ^2_i u_i.$$\n",
    "That is, $u_i$ are eigen vectors of $AA^T$ for $i=1,...k$.\n",
    "* We extend $\\{u_i\\}^k_{i=1}$ to an orthonormal basis for all of $F^m$, by adding orthonormal vectors in the null space of $u_i$ for $i=1,...k$, and get\n",
    "$\\{u_i\\}^m_{i=1}$.\n",
    "* Let\n",
    "$$\\begin{align}\n",
    "U &\\equiv (u_1 · · · u_m), \\; \\text{and} \\\\\n",
    "V &\\equiv (v_1 · · · v_n) \n",
    "\\end{align}$$\n",
    "That is, $U$ is the matrix with $u_i$ as columns and $V$ is the matrix with $v_i$ as columns. Then\n",
    "$$U^TAV = \\begin{pmatrix}\n",
    "u^T_1\\\\\n",
    ". \\\\\n",
    ". \\\\\n",
    ". \\\\\n",
    "u^T_k \\\\\n",
    ". \\\\\n",
    ". \\\\ \n",
    ". \\\\\n",
    "u^T_m\\end{pmatrix}\n",
    "A \n",
    "\\begin{pmatrix} v_1 · · · v_n \\end{pmatrix} \\\\\n",
    "= \\begin{pmatrix}\n",
    "u^T_1\\\\\n",
    ". \\\\\n",
    ". \\\\\n",
    ". \\\\\n",
    "u^T_k \\\\\n",
    ". \\\\\n",
    ". \\\\ \n",
    ". \\\\\n",
    "u^T_m\\end{pmatrix}\n",
    "\\begin{pmatrix}σ_1u_1 · · · σ_k u_k \\, 0 · · · 0 \\end{pmatrix} \n",
    "= \\begin{pmatrix}\n",
    "σ & 0 \\\\\n",
    "0 & 0 \\end{pmatrix}\n",
    "$$\n",
    "where $\\sigma$ is given in the statement of the theorem. \u0004"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "izCSh9hfvs-u"
   },
   "source": [
    "## Phyical Interpretation of SVD\n",
    "\n",
    "* Given any data vector ${a_1}$, we can project it on some unit vector ${v_1}$ by doing \n",
    "$$ {a_1} \\cdot {v_1} = s_{11} $$\n",
    "* If we have two orthogonal unit vectors, and we want to project $a_1$ to these two vectors at one shot, we can express this as\n",
    "$$ {a_1} \\cdot [{v_1} v_2 ] = [s_{11} s_{12}] $$\n",
    "* If we have many data vectors $a_1, a_2, ..., a_m$, and many orthogonal vectors$v_1, v_2, v_n$, we can do projections of all data points to all unit vectors at one shot:\n",
    "$$\\begin{bmatrix} a_1\\\\.\\\\ . \\\\a_m \\end{bmatrix}\n",
    "\\begin{bmatrix} v_1 ... v_n \\end{bmatrix} = \n",
    "\\begin{bmatrix} s_{11} ... s_{1n} \\\\\n",
    "... \\\\ s_{m1} ... s_{mn} \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Write in matrix notation, we have:\n",
    "$$ A V = S $$\n",
    "* If we look at the columns of $S$, each one of them may have very different scales.  We can normalize it with the \"length\" of each column, so that it is easier to compare or to do other processing.  For each column $j$\n",
    "$$ \\begin{bmatrix} s_{1j} \\\\ . \\\\ . \\\\ s_{mj} \\end{bmatrix}$$\n",
    "we divide each element by its vector length $\\sqrt{s_{1j}^2+...+s_{mj}^2}$.  We call this length $\\sigma_j$.  Then, we have\n",
    "$$ S = \\begin{bmatrix} \n",
    "\\frac{s_{11}}{\\sigma_1} ... \\frac{s_{1n}}{\\sigma_n} \\\\\n",
    "... \\\\ \\frac{s_{m1}}{\\sigma_1} ... \\frac{s_{mn}}{\\sigma_n}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix} \\sigma_1 ... 0 \\\\\n",
    "... \\\\ 0 ... \\sigma_n \n",
    "\\end{bmatrix} = U \\Sigma\n",
    "$$\n",
    "That is, $ AV = U\\Sigma$, and we have something very similar to SVD: $$A = U\\Sigma V^T$$\n",
    "* However, up to this point, we only assume that $V$ is a set of orthonormal vectors, and $U$ is normalized to become a set of unit vectors.  Note that\n",
    "  * There can be infinite many sets of $V$.\n",
    "  * At this point, we can use any set of $V$ that is useful for our purpose.  \n",
    "  * The column vectors in $U$ though being unit vectors, are not necessarily orthogonal (independent) to one another.  That is, they are not orthonormal.\n",
    "* If for our purpose, we want $U$ to be orthonormal, too, then we are looking for SVD. In this case, we have to find a particular set of $V$ that can enable $U$ to be orthonormal.  The theorem of singular value decomposition proves that that the solution always exist, and is a unique solution.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LZac_ght6alv"
   },
   "source": [
    "## Essence of Principle Component Analysis (PCA)\n",
    "In singular value decomposition, we have \n",
    "$$ A = U\\Sigma V^T$$\n",
    "where $\\Sigma$ is an diagonal matrix, with the singular values order from big to small.  If we have a diagonal matrix \n",
    "$$ \\Sigma = diag(\\sigma_1, \\sigma_2, ..., \\sigma_n),$$\n",
    "And the first few eigenvalues are much larger than the last few eigenvalues, we decide to approximate this diagonal matrix by setting the eigenvalues after index $k$ to be 0.  That is, we approximate $\\Sigma$ by \n",
    "$$ \\Sigma' = diag(\\sigma_1, \\sigma_2, ..., \\sigma_k, 0, 0, ...)$$\n",
    "\n",
    "This is the essence of principal component analysis.  "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "uiAlADDc7IJH",
    "53TGYyIb7zac",
    "DmU24ltr8eCo",
    "ztMOPCFJePbp"
   ],
   "name": "LinAlgebra_Decomposition.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
