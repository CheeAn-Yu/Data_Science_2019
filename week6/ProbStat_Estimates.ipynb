{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e_shGVru1WnB"
   },
   "source": [
    "# Estimates \n",
    "\n",
    "\n",
    "## Point estimates\n",
    "* Parameter space\n",
    "* Statistic → Estimate\n",
    "* Moment method (Pearson)\n",
    "  * Population moments equate to sample moments\n",
    "  * If you have n parameters, equate up to moment n, and solve the equation system.\n",
    "\n",
    "## Maximum Likelihood Method (Sir Ronald Fisher)\n",
    "* Maximum likelihood function L = product of probability of all sample, which is the probability of the sample distribution, assuming all samples are independent.\n",
    "* Find the $\\theta$ that maximizes L\n",
    "\n",
    "\n",
    "\n",
    "## Bayesian Method\n",
    "* Classic: theta is described by a value\n",
    "* Bayes: theta is described by a probability distribution (Prior dist)\n",
    "  * As data are collected, the distribution is updated → post dist\n",
    "  * The update method is based on Bayes theorem\n",
    "\n",
    "\n",
    "**Definition 15.7.**\n",
    "* Let X1, X2, ..., Xn be a random sample from a distribution\n",
    "with density $f(x/\\theta)$, where $\\theta$ is the unknown parameter to be estimated. \n",
    "* Let $\\hat{\\theta}$ b be an estimator of $\\theta$. \n",
    "* Define:\n",
    "  * The squared error loss:\n",
    "$$\\mathcal{L_2}( \\hat{\\theta}, \\theta) = (\\hat{\\theta} - \\theta)^2$$ \n",
    "  * The absolute error loss: \n",
    "$$ \\mathcal{L_1(\\hat{\\theta}}, \\theta) = |\\hat{\\theta} - \\theta)|$$\n",
    "\n",
    "**Definition 15.8**\n",
    "* Let X1, X2, ..., Xn be a random sample from a distribution\n",
    "with density $f(x/\\theta)$, where $\\theta$ is the unknown parameter to be estimated. \n",
    "* Let $\\hat{\\theta}$ b be an estimator of $\\theta$. \n",
    "* Let $\\mathcal{L(\\hat{\\theta}, \\theta)}$ be a loss function.\n",
    "* Define **risk** as\n",
    "   $$ \\mathcal{R_L} = \\int{ \\mathcal{L(\\hat{\\theta},\\theta)} \\space f(x/\\theta) \\space dx  }   $$\n",
    "\n",
    "What's the use of **risk**?  \n",
    "\n",
    "### Determine $\\hat{\\theta}$ using Basyes method\n",
    "* Given conditional distribution $f(x/\\theta)$, and given prior belief $h(\\theta)$, we can obtain the conditional density of the parameter $k(\\theta|x_1, x_2, \\ldots, x_n)$\n",
    "* The Basyesian method says that we want to get an estimate that minimize the expectaton of the less function.  That is, minimizying the integral:\n",
    "  $$ \\int_\\Omega \\mathcal{L(\\hat{\\theta},\\theta)} \\space k(\\theta|x_1, x_2, \\ldots, x_n) \\space dx $$\n",
    "  where $\\Omega$ is the support of prior density of the parameter $h(\\theta)$\n",
    "\n",
    "\n",
    "* If we use $ \\mathcal{L(\\hat{\\theta},\\theta)} = (\\hat{\\theta} - \\theta)^2$, then the above formula to minimize is just the conditional expectation of variance \n",
    "$$ E((\\hat{\\theta} - \\theta)^2 | x_1, x_2, \\ldots, x_n) $$\n",
    "\n",
    "The following theorem is based on the fact that the function  defined by \n",
    "$$\\phi(c) = E((X - c)^2$$\n",
    "attains minimum if $c = E[X]$.\n",
    "\n",
    "**Theorem 15.3. When loss function is square error**\n",
    "* Let X1, X2, ..., Xn be a random sample from a distribution\n",
    "with density $f(x|\\theta)$, where $\\theta$ is the unknown parameter to be estimated. \n",
    "* If the loss function is squared error, then the Bayes’ estimator $\\hat{\\theta}$ of parameter $\\theta$ is given by\n",
    "$$ \\hat{\\theta} = E(\\theta|x1, x2, ..., xn),$$\n",
    "where the expectation is taken with respect to density $k(\\theta|x1, x2, ..., xn)$.\n",
    "\n",
    "**Theorem 15.4. When the lass function is absolute error**\n",
    "* Let X1, X2, ..., Xn be a random sample from a distribution\n",
    "with density $f(x|\\theta)$, where$\\theta$ is the unknown parameter to be estimated. \n",
    "* If the loss function is absolute error, then the Bayes estimator $\\hat{\\theta}$ of the parameter $\\theta$ is given by\n",
    "$$ \\hat{\\theta} = median \\space of \\space k(\\theta|x1, x2, ..., xn)$$\n",
    "where $k(\\theta|x1, x2, ..., xn)$ is the posterior distribution of $\\theta$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bx6D_J1xCKKN"
   },
   "source": [
    "## The goodness of point estimators\n",
    "Some well known criteria for evaluating the goodness of an estimator are: \n",
    "1. Unbiasedness, \n",
    "2. Eciency and Relative Efficiency, \n",
    "3. Uniform Minimum Variance Unbiasedness, \n",
    "4. Sufficiency, and \n",
    "5. Consistency.\n",
    "\n",
    "For formal definition of estimator and estimates:\n",
    "* Let X1, X2, ..., Xn be a random sample of size n from a population with\n",
    "probability density function $f(x; \\theta)$. \n",
    "* An estimator $\\hat{\\theta}$ of $\\theta$ is a function of the random variables X1, X2, ..., Xn which is free of the parameter $\\theta$. \n",
    "* An estimate is a realized value of an estimator that is obtained when a sample\n",
    "is actually taken\n",
    "\n",
    "**Definition 16.1. Unmiased estimator**\n",
    "* An estimator $\\hat{\\theta}$ of $\\theta$ is said to be an unbiased estimator of\n",
    "$\\theta$ if and only if\n",
    "$$ E({\\hat{\\theta}}) = \\theta $$\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ProbStat - Estimates.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
