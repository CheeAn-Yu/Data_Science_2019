{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dESRHAeSkPAO"
   },
   "source": [
    "# Special Continuous Distribution Functions\n",
    "\n",
    "## Uniform distribution function\n",
    "\n",
    "**Definition 6.1. Uniform**\n",
    "* A random variable X is said to be uniform on the interval $[a, b]$ if its probability density function is of the form\n",
    "$$ f(x) = \\frac{1}{b-a}, \\quad a \\le x \\le b,$$\n",
    "where $a$ and $b$ are constants. \n",
    "* We denote a random variable X with the uniform distribution on the interval $[a, b]$ as $$X \\sim UNIF(a, b).$$\n",
    "\n",
    "**Theorem 6.1.**\n",
    "* If X is uniform on the interval $[a, b]$ then the mean, variance\n",
    "and moment generating function of X are given by\n",
    "$$ E(X) = \\frac{b + a}{2}$$\n",
    "$$ Var(X) = \\frac{(b - a)^2}{12}$$\n",
    "$$ M(t) = \n",
    "\\begin{cases} 1 & if \\space t = 0, \\\\\n",
    "\\space \\frac{e^{tb}-e^{ta}}{ t(b-a)} & if \\space t \\ne 0 \n",
    "\\end{cases}$$\n",
    "\n",
    "\n",
    "**Theorem 6.2. CDF as random variable. Important!**\n",
    "* If X is a continuous random variable with a strictly increasing\n",
    "cumulative distribution function F(x)\n",
    "* Then the random variable Y , defined by $Y = F(X)$ has the uniform distribution on the interval $[0, 1]$.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AwOrxj9GoR9y"
   },
   "source": [
    "## Gamma Distribution\n",
    "\n",
    "**Gamma Function**\n",
    "The gamma function is defined as\n",
    "$$\\Gamma(z) := \\int_0^\\infty{x^{z-1}e^{-x}dx},$$\n",
    "\n",
    "for all $ z \\in R - \\{0, -1, -2, -3, ...\\}$\n",
    "\n",
    "The integral onthe right hand side of the above equation is called \"Euler's second integral\".\n",
    "\n",
    "**Lemma 6.1.**\n",
    "$$ \\Gamma (1) = 1.$$\n",
    "Proof:\n",
    "$$\\begin{align}\n",
    "\\Gamma(1) &= \\int_0^\\infty x^0 e^{-x} dx \\\\\n",
    "  &= \\Bigl[-e^{-x}\\Bigr]_0^\\infty = 1.\n",
    "\\end{align}$$\n",
    "\n",
    "**Lemma 6.2.**\n",
    "* The gamma function (z) satisfies the functional equation\n",
    "$$\\Gamma(z) = (z-1)\\Gamma(z-1)$$ for all real number $z > 1.$\n",
    "\n",
    "**Proof:**\n",
    "* Let z be a real number such that $z > 1$, and consider\n",
    "$$\\begin{align}\n",
    "\\Gamma(z) &= \\int_0^\\infty x^{z-1} e^{-x} dx \\quad \\text{ --> use integration by parts,} \\\\\n",
    "  &= [-x^{z-1} e^{-x}]_0^\\infty + \\int_0^\\infty (z-1) x^{z-2} e^{-x} dx \\\\\n",
    "  &= (z - 1) \\int_0^\\infty x^{z-2} e^{-x} dx \\\\\n",
    "  &= (z-1) \\Gamma(z-1).\n",
    "\\end{align}$$\n",
    "\n",
    "**Lemma 6.3.**\n",
    "$$\\Gamma(\\frac{1}{2}) = \\sqrt{\\pi}$$\n",
    "\n",
    "**Proof:**\n",
    "* Involved transformation to angular reference frame, and skipped.  Shall come back later with more calculas techniques.\n",
    "\n",
    "**Lemma 6.4.**\n",
    "$$\\Gamma(-\\frac{1}{2}) = -2\\sqrt{\\pi}$$\n",
    "\n",
    "**Proof:**\n",
    "* From\n",
    "$$ \\Gamma(z) = (z-1) \\Gamma(z-1)$$\n",
    "* Let $z = 1/2$\n",
    "$$\\begin{align}\n",
    "\\Gamma(\\frac{1}{2}) &= (\\frac{1}{2} - 1) \\Gamma(\\frac{1}{2} - 1) \\\\\n",
    "  &= -\\frac{1}{2} \\Gamma(-\\frac{1}{2}) \\\\\n",
    "\\Gamma(-\\frac{1}{2}) &= -2\\Gamma(\\frac{1}{2}) \\\\\n",
    "  &= -2 \\sqrt{\\pi}\n",
    "\\end{align}$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Definition 6.2. Gamma Distribution**\n",
    "* A continuous random variable X is said to have a gamma distribution if its probability density function is given by\n",
    "$$ f(x) = \n",
    "\\begin{cases}\n",
    "\\frac{1}{\\Gamma(\\alpha)\\theta^\\alpha} x^{\\alpha-1} e^{-\\frac{x}{\\theta}}, & if \\space 0 < x < \\infty \\\\\n",
    "   0 & otherwise,\n",
    "\\end{cases}$$\n",
    "\n",
    "where  $\\alpha > 0$ and $\\theta > 0$. \n",
    "* We denote a random variable with gamma distribution as $X \\sim GAM(\\theta, \\alpha)$. \n",
    "\n",
    "**Analysis:**\n",
    "* Since $$\\Gamma(z) := \\int_0^\\infty{x^{z-1}e^{-x}dx},$$\n",
    "If we let $z=\\alpha$, and $x = \\frac{y}{\\theta}$, we have $dx = \\frac{1}{\\theta} dy$, and the above formula becomes:\n",
    "$$\\begin{align}\n",
    "\\Gamma(\\alpha) &:= \\int_0^\\infty{(\\frac{y}{\\theta})^{\\alpha-1}e^{-\\frac{y}{\\theta}} \\frac{1}{\\theta} dy} \\\\\n",
    "  &= \\int_0^\\infty \\frac{1}{\\theta^\\alpha} y^{\\alpha-1} e^{-\\frac{y}{\\theta}} dy \\quad \\text{-->Divide both side by } \\Gamma(\\alpha)\\\\\n",
    "1 &= \\int_0^\\infty \\frac{1}{\\Gamma(\\alpha)\\: \\theta^\\alpha} y^{\\alpha-1} e^{-\\frac{y}{\\theta}} dy\n",
    "\\end{align}$$ \n",
    "* We can see that the right-hand-side inside the integral is a density function.  This is the **Gamma distribution function**.\n",
    "\n",
    "**Theorem 6.3.**\n",
    "*  If $X \\sim GAM(\\theta, \\alpha)$, then\n",
    "$$ \\begin{align}\n",
    "E(X) &= \\theta \\alpha \\\\\n",
    "Var(X) &= \\theta^2 \\alpha \\\\\n",
    "M(t) &= \\frac{1}{(1 - \\theta t)^\\alpha} \\quad for \\space t<\\theta\n",
    "\\end{align}$$\n",
    "\n",
    "**Proof:**\n",
    "* Derive the moment generating function. \n",
    "* Then \n",
    "  * $E(x) = M^\\prime(0)$ and \n",
    "  * $Var(X)=M^{\\prime\\prime}(0) - (M^\\prime(0))^2$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7zYbZLMV0RWO"
   },
   "source": [
    "## Special cases of Gamma Function\n",
    "\n",
    "### Exponential function as special case of Gamma\n",
    "**Definition 6.3. Exponential**\n",
    "* A continuous random variable is said to be an exponential random variable with parameter $\\theta$ if its probability density function is of the form\n",
    "$$ f(x) = \\begin{cases}\n",
    "  \\frac{1}{\\theta} e^{-\\frac{x}{\\theta}}, & for \\space x>0  \\\\\n",
    "  0 & otherwise\n",
    "\\end{cases}$$\n",
    "where $\\theta > 0$. \n",
    "* If a random variable X has an exponential density function with parameter $\\theta$, then we denote it by writing $X \\sim EXP(\\theta)$.\n",
    "\n",
    "**Analysis:**\n",
    "* The exponential density function is simply the Gamma density function with $\\alpha=1$:\n",
    "$$ Gamma(\\theta, 1) \\sim \\frac{1}{\\Gamma(1)\\theta^1} x^0 e^{-\\frac{x}{\\theta}} = \\frac{1}{\\theta} e^{-\\frac{x}{\\theta}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7i_JLLfx8Rij"
   },
   "source": [
    "### Chi-square and n-Erling as special cases of Gamma\n",
    "\n",
    "**Definition 6.4. Chi-square**\n",
    "* A continuous random variable X is said to have a chi-square distribution with r degrees of freedom if its probability density function is of the form\n",
    "$$ f(x) = \\begin{cases} \n",
    "\\frac{1}{\\Gamma(\\frac{r}{2}) 2^\\frac{r}{2}} x^{\\frac{r}{2} -1}e^{-\\frac{x}{2}}  dx &\\text{if }0 < x < \\infty, \\\\\n",
    "0 & otherwise,\n",
    "\\end{cases}$$\n",
    "* If X has a chi-square distribution, then we denote it by writing $X \\sim \\chi^2(r)$.\n",
    "\n",
    "**Analysis**\n",
    "* $\\chi^2(r)$ is just $\\Gamma(2, \\frac{r}{2})$\n",
    "* That is, Gamma density function with $\\theta=2$ and $\\alpha = \\frac{r}{2}$\n",
    "\n",
    "Note: if $r \\rightarrow \\infty $, then the chi-square distribution tends to\n",
    "the normal distribution. (Proof omitted.)\n",
    "\n",
    "The chi-square distribution was originated in the works of British Statistician Karl Pearson (1857-1936) but it was originally discovered by German\n",
    "physicist F. R. Helmert (1843-1917).\n",
    "\n",
    "**Definition 6.5. n-Erlang**\n",
    "* A continuous random variable X is said to have a n-Erlang distribution if its probability density function is of the form\n",
    "$$ f(x) = \\lambda e^{-\\lambda x}\\frac{(\\lambda x)^{n-1}}{(n-1)!} $$\n",
    "or equivalently,\n",
    "$$ f(x) = \\frac{1}{(n-1)!} \\lambda^n x^{n - 1} e^{-\\lambda x}$$\n",
    "\n",
    "if $0 < x < \\infty$, and $f(x)=0$ otherwise,\n",
    "where  $\\lambda > 0$ is a parameter.\n",
    "\n",
    "**Analysis**\n",
    "* n-Erlang distribution is just $\\Gamma(\\frac{1}{\\lambda}, n)$\n",
    ",that is, $\\theta = \\frac{1}{\\lambda}$ and $\\alpha=n$, where $n$ is a positive integer.\n",
    "$$ \\begin{align} \n",
    "\\Gamma(\\frac{1}{\\lambda}, n) &= \\frac{\\lambda^n}{\\Gamma(n) } x^{n-1} e^{-\\lambda x} \\\\\n",
    "  &= \\frac{\\lambda^n}{(n-1)! }    x^{n-1} e^{-\\lambda x} \\\\\n",
    "  &= \\lambda e^{-\\lambda x} \\frac{(\\lambda x)^{n-1}}{(n-1)!}\n",
    "\\end{align}$$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w3Jbo7JERWJr"
   },
   "source": [
    "## Unified distribution and the Weibull distribution \n",
    "\n",
    "**Definition: The unified distribution**\n",
    "* The unified distribution functions is defined as:\n",
    "$$ f(x) = \\begin{cases}\n",
    "\\frac{\\alpha}{ \\Gamma(\\alpha^\\psi + 1) \\theta^{\\alpha^\\psi}} \n",
    "          x^{\\alpha-1} e^\\frac{-x^{-(\\alpha^\\psi - \\alpha - 1)}}{\\theta} \n",
    "  &\\text{if } 0 < x <\\infty, \\\\ \n",
    "0 &otherwise,\n",
    "\\end{cases}$$ \n",
    "where $\\theta > 0$, $\\alpha > 0$, and $\\psi \\in \\{0, 1\\}$ are parameters.\n",
    "\n",
    "**Observation**\n",
    "* If $\\psi = 1$, the unified distribution becomes the Gamma distribution.\n",
    "\n",
    "**Analysis**\n",
    "* With $\\psi = 1$, the unified distribution becomes:\n",
    "$$\\begin{align}\n",
    "f(x) &=   \\frac{\\alpha}{ \\Gamma(\\alpha + 1) \\theta^{\\alpha}} \n",
    "          x^{\\alpha-1} e^\\frac{-x^{-(\\alpha - \\alpha - 1)}}{\\theta} \\\\\n",
    "  &= \\frac{1}{ \\Gamma(\\alpha) \\theta^{\\alpha}} \n",
    "          x^{\\alpha-1} e^\\frac{-x}{\\theta}  \n",
    "\\end{align}$$\n",
    "Note: $\\Gamma(\\alpha+1) = \\alpha \\: \\Gamma(\\alpha)$\n",
    "\n",
    "**Definition: Weibull distribution**\n",
    "* Weibull distribution is defined as:\n",
    "$$ f(x) = \\begin{cases} \n",
    "  \\frac{\\alpha}{\\theta} x^{\\alpha-1} e^{-\\frac{x^\\alpha}{\\theta}} &\\text{if }0 < x <\\infty \\\\\n",
    "  0 &otherwise\n",
    "\\end{cases}$$\n",
    "\n",
    "**Analysis**\n",
    "* The Weibull distribution is just the unified distribution with $\\psi = 0$:\n",
    "$$\\begin{align}\n",
    " f(x) &= \\frac{\\alpha}{ \\Gamma(\\alpha^0 + 1) \\theta^{\\alpha^0}} \n",
    "          x^{\\alpha-1} e^\\frac{-x^{-(\\alpha^0 - \\alpha - 1)}}{\\theta} \\\\\n",
    "  &= \\frac{\\alpha}{ \\Gamma(2) \\theta} \n",
    "          x^{\\alpha-1} e^\\frac{-x^{\\alpha}}{\\theta} \\\\\n",
    "  &= \\frac{\\alpha}{\\theta} x^{\\alpha-1} e^\\frac{-x^{\\alpha}}{\\theta}\n",
    "\\end{align}$$\n",
    "Note that $\\Gamma(2) = \\Gamma(1) = 1$.\n",
    "* For $\\alpha = 1$, the Weibull distribution becomes an exponential distribution. \n",
    "* The Weibull distribution provides probabilistic models for life-length data of components or systems.\n",
    "\n",
    "The mean and variance of the Weibull distribution:\n",
    "* Mean:\n",
    "  $$ E(x) = \\theta^\\frac{1}{\\alpha} \\Gamma(1+\\frac{1}{\\alpha})  $$\n",
    "* Variance:\n",
    "  $$ Var(x) = \\theta^\\frac{2}{\\alpha} \n",
    "  \\{ \\Gamma(1+\\frac{2}{\\alpha}) - (1+\\frac{1}{\\alpha})^2) \\} $$\n",
    "\n",
    "**Definition: Rayleigh distribution**\n",
    "* Rayleigh distribution is\n",
    "  * The unified distribution with $\\psi = 0$, $\\theta = 2\\sigma^2$, and $\\alpha = 2$, or equivalently, \n",
    "  * Weibull distribution with $\\theta = 2\\sigma^2$ and $\\alpha = 2$\n",
    "* That is, the Rayleigh distribution is:\n",
    "$$ f(x) = \\begin{cases}\n",
    "\\frac{x}{\\sigma^2} e^{-\\frac{x^2}{2\\sigma^2}} &\\text{if }0 < x <\\infty\\\\\n",
    " 0 &otherwise.\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a3nSG51MQFrQ"
   },
   "source": [
    "## Beta distribution\n",
    "* One of the basic distributions in statistics. \n",
    "* Has many applications in classical and Bayesian statistics\n",
    "* Used in modeling the behavior of random variables that are positive but bounded in possible values. \n",
    "* Proportions and percentages fall in this category.\n",
    "\n",
    "### Beta function\n",
    "The beta function $B(\\alpha, \\beta)$ is defined as:\n",
    "$$ B(\\alpha, \\beta) = \\int_0^1 x^{\\alpha-1} (1-x)^{\\beta-1} dx $$\n",
    "\n",
    "The following theorem connects the beta function with the gamma function.\n",
    "\n",
    "**Theorem 6.4. Beta function and Gamma function**\n",
    "* Let $\\alpha$ and $\\beta$ be any two positive real numbers. Then\n",
    "$$ B(\\alpha, \\beta) = \\frac{ \\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)}  $$\n",
    "\n",
    "where $ \\Gamma(z) = \\int_0^\\infty x^{z-1} e^{-x} dx$ is the gamma function.\n",
    "\n",
    "**Proof:**\n",
    "$$ \\begin{align}\n",
    "\\Gamma(\\alpha) \\Gamma(\\beta) &=\n",
    "(\\int_0^\\infty x^{\\alpha-1}e^{-x}dx)(\\int_0^\\infty y^{\\beta-1} e^{-y} dy), \\quad let\\space x=u^2, y=v^2 \\\\\n",
    "  &= (\\int_0^\\infty u^{2\\alpha-2} e^{-u^2} 2u \\, du)(\\int_0^\\infty v^{2\\beta-2} e^{-v^2} 2v \\, dv) \\\\\n",
    "  &= 4 \\int_0^\\infty\\int_0^\\infty u^{2\\alpha-1}v^{2\\beta-1} e^{-(u^2+v^2)} du\\, dv, \\quad let \\space u=r\\thinspace cos\\,\\theta, v=r \\thinspace sin\\,\\theta\\\\\n",
    "  &= 4 \\int_0^{2\\pi} \\int_0^\\infty (r^2)^{\\alpha+\\beta-1} (cos\\,\\theta)^{2\\alpha-1}(sin\\,\\theta)^{2\\beta-1} e^{-r^2} r\\,dr\\,d\\theta \\\\\n",
    "  &= ( \\int_0^\\infty (r^2)^{\\alpha+\\beta-1}\\,dr^2)(2\\int_0^{2\\pi} (cos\\,\\theta)^{2\\alpha-1}(sin\\,\\theta)^{2\\beta-1}\\,d\\theta) \\\\\n",
    "  &= \\Gamma(\\alpha+\\beta) (2\\int_0^{2\\pi} (cos\\,\\theta)^{2\\alpha-1}(sin\\,\\theta)^{2\\beta-1}\\,d\\theta), \\: let \\: t=cos^2\\theta \\\\\n",
    "  &= \\Gamma(\\alpha+\\beta) \\int_0^1 t^{\\alpha-1} (1-t)^{\\beta-1} dt \\\\\n",
    "  &= \\Gamma(\\alpha+\\beta) B(\\alpha,\\beta) \n",
    "\\end{align}$$\n",
    "\n",
    "Note that in the above proof, if $t=cos^2\\theta$, then we have $dt = 2cos\\theta \\, sin \\theta \\, d\\theta$\n",
    "\n",
    "\n",
    "**Corollary 6.1:**\n",
    "$$ B(\\alpha,\\beta) = B(\\beta,\\alpha) $$\n",
    "**Corollary 6.2**\n",
    "The beta function can be written as:\n",
    "$$ B(\\alpha,\\beta) = 2 \\int_0^{2\\pi} cos\\,\\theta)^{2\\alpha-1}(sin\\,\\theta)^{2\\beta-1}\\,d\\theta $$\n",
    "\n",
    "**Corollary 6.3:**\n",
    "By substituting $s = \\frac{t}{1-t}$, the beta function can alos be written in the following form:\n",
    "$$ B(\\alpha,\\beta) = \\int_0^\\infty \\frac{s^{\\alpha-1}}{(1+s)^{\\alpha+\\beta}} ds$$\n",
    "\n",
    "**Corollary 6.4:**\n",
    "For every positive real number $\\beta$ and every positive integer $\\alpha$, the beta function reduces to\n",
    "$$ B(\\alpha,\\beta) = \\frac{(\\alpha-1)!}{(\\alpha-1+\\beta)(\\alpha-2+\\beta)...(1+\\beta)\\beta}$$\n",
    "\n",
    "**Corollary 6.5.**\n",
    "For every pair of positive integers $\\alpha$ and $\\beta$, the beta function satisfies the following recursive relation\n",
    "$$ B(\\alpha,\\beta) = \\frac{(\\alpha-1)(\\beta-1)}{(\\alpha+\\beta-1)(\\alpha+\\beta-2)} B(\\alpha-1,\\beta-1)$$\n",
    "\n",
    "**Definition 6.6. Beta density function**\n",
    "* A random variable X is said to have the beta density function if its probability density function is of the form, for every positive $\\alpha$ and $\\beta$:\n",
    "$$ f(x) = \n",
    "\\begin{cases}\n",
    "\\frac{1}{B(\\alpha, \\beta)}x^{\\alpha-1}(1-x)^{\\beta-1}, & for \\: 0 < x < 1  \\\\\n",
    "  0 & otherwise\n",
    "\\end{cases}\n",
    "$$\n",
    "* If X has a beta distribution, then we write $X \\sim BETA(\\alpha,\\beta)$\n",
    "\n",
    "Note: The beta distribution reduces to the uniform distribution over (0, 1), if\n",
    "$\\alpha = 1 = \\beta$. (Proof requires a complicated limiting process.)\n",
    "\n",
    "**Theorem 6.5. Mean and variance of beta distribution**\n",
    "* If $X \\sim BETA(\\alpha,\\beta)$, then\n",
    "  * $$ E(X) = \\frac{\\alpha}{\\alpha+\\beta} $$\n",
    "  * $$ Var(X) = \\frac{\\alpha\\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)}$$\n",
    "\n",
    "**Proof:**\n",
    "* Use theorem 6.4.\n",
    "\n",
    "### Generalized beta distribution\n",
    "* The beta distribution can be generalized to any bounded interval [a, b].\n",
    "This generalized distribution is called the generalized beta distribution. \n",
    "* If a random variable X has this generalized beta distribution we denote it by\n",
    "writing $X \\sim GBETA(\\alpha, \\beta, a, b)$. \n",
    "* The probability density of the generalized beta distribution is given by\n",
    "$$ f(x) =\n",
    "\\begin{cases}\n",
    "  \\frac{1}{B(\\alpha,\\beta)} \\frac{(x-a)^{\\alpha-1} (b-x)^{\\beta-1}}{(b-a)^{\\alpha+\\beta-1}}&  if \\: a < x < b,\\\\\n",
    "  0 & otherwise\n",
    "\\end{cases}  \n",
    "$$\n",
    "where $\\alpha,\\beta, a > 0$\n",
    "\n",
    "**Theorem**\n",
    "If $X \\sim GBETA(\\alpha,\\beta, a, b)$ then\n",
    "$$ E(X) = (b-a)\\frac{\\alpha}{\\alpha+\\beta} + a $$\n",
    "$$ Var(X) = (b-a)^2 \\frac{\\alpha\\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)}$$\n",
    "\n",
    "**Theorem**\n",
    "It can be shown that if $X = (b - a)Y + a$ and $Y \\sim BETA(\\alpha, \\beta)$, then $X \\sim GBETA(\\alpha, \\beta, a, b)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vCn_6PXT1xIe"
   },
   "source": [
    "## Normal distribution\n",
    "\n",
    "### History\n",
    "* discovered by a French mathematician Abraham DeMoivre (1667-1754).\n",
    "  * DeMoivre wrote two important books. One is called the **Annuities Upon\n",
    "Lives**, the first book on actuarial sciences and the second book is called the **Doctrine of Chances**, one of the early books on the probability theory. \n",
    "* PierreSimon Laplace (1749-1827) applied normal distribution to astronomy. \n",
    "* Carl Friedrich Gauss (1777-1855) used normal distribution in his studies of problems in physics and astronomy. \n",
    "* Adolphe Quetelet (1796-1874) demonstrated that man�䏭 physical traits (such as height, chest expansion, weight etc.) as well as social traits follow normal distribution. \n",
    "\n",
    "The main importance of normal distribution lies on the central limit theorem which says that the **sample mean has a normal distribution** if the sample size is large.\n",
    "\n",
    "**Definition 6.7.**\n",
    "* A random variable X is said to have a normal distribution\n",
    "if its probability density function is given by\n",
    "$$\n",
    "\\begin{align}\n",
    " f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{\\frac{1}{2}(-\\frac{x-\\mu}{\\sigma})^2},    && -\\infty < x < \\infty \n",
    "\\end{align} \n",
    "$$\n",
    "where $-\\infty < \\mu < \\infty$ and $0 < \\sigma^2 < \\infty$ are arbitrary parameters. \n",
    "* If X has a normal distribution with parameters $\\mu$ and $\\sigma^2$, then we write $X \\sim N(\\mu, \\sigma^2)$.\n",
    "\n",
    "**Analysis: Show that Normal distribution is a probability density function:**\n",
    "* Assume X has normal distribution, We integrate $f(x)$ over all real numbers.  First, we make the following variable assignment:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "z &= \\frac{1}{2}(\\frac{x-\\mu}{\\sigma})^2 \\\\\n",
    "x &= \\pm (\\sigma \\sqrt{2z} + \\mu) \\\\\n",
    "dx &= \\pm(\\sqrt{2}\\sigma \\frac{1}{2} z^{-\\frac{1}{2}} )dz\\\\\n",
    "dx &= \\frac{\\sigma}{\\sqrt{2z}}dz\n",
    "\\end{align}\n",
    "$$\n",
    "Then, we have:\n",
    "$$\n",
    "\\begin{align}\n",
    " \\int_{-\\infty}^\\infty f(x) dx &= \n",
    " \\int_{-\\infty}^\\infty \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{1}{2}(\\frac{x-\\mu}{\\sigma})^2} dx\\\\\n",
    " &= \\int_{-\\infty}^\\infty \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-z} \\frac{\\sigma}{\\sqrt{2z}}dz \\\\\n",
    " &= \\int_{-\\infty}^\\infty \\frac{1}{2 \\sqrt{\\pi}} e^{-z} \\frac{1}{\\sqrt{z}}dz \\\\\n",
    " &= \\frac{1}{\\sqrt{\\pi}} \\int_0^\\infty z^{-\\frac{1}{2}}e^{-z}dz,  \\\\\n",
    " &= \\frac{1}{\\sqrt{\\pi}} \\Gamma(\\frac{1}{2}) = \\frac{1}{\\sqrt{\\pi}} \\sqrt{\\pi} = 1\n",
    "\\end{align} \n",
    "$$\n",
    "\n",
    "**Theorem 6.6.**\n",
    "* If $x \\sim N(\\mu, \\sigma^2)$, then\n",
    "$$ \\begin{align}\n",
    "E(X)   &= \\mu  \\\\\n",
    "Var(X) &= \\sigma^2  \\\\\n",
    "M(t)   &= e^{繕t+ \\frac{1}{2}\\sigma^2t^2}\n",
    "\\end{align} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ProbStat_continuous_distributions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
